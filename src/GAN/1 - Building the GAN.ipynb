{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/gan.py\n",
    "https://stackoverflow.com/questions/40994583/how-to-implement-tensorflows-next-batch-for-own-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "\n",
    "def get_labels_dict(data_path):\n",
    "    labels_dict = {}\n",
    "    with open(data_path + 'sci_labels.csv', 'r') as f:\n",
    "        file = csv.reader(f)\n",
    "        for row in file:\n",
    "            labels_dict[row[0]] = row[1]\n",
    "    return labels_dict\n",
    "\n",
    "\n",
    "def get_features_labels(root, labels_dict):\n",
    "    corpus = [] # each row is a string formed from all messages in a conversations\n",
    "    labels = [] # each row is 0 or 1, corresponds to label for same row in corpus\n",
    "\n",
    "    for conversation in root:\n",
    "        string = \" \"\n",
    "        for message in conversation:\n",
    "            text = message.find('text').text\n",
    "            if text is not None:\n",
    "                string = string + \"\\r\\n\" + text \n",
    "        corpus.append(string)\n",
    "        labels.append(int(labels_dict[conversation.get('id')]))\n",
    "    return corpus, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '../../data/svm_training_data/'\n",
    "training_xml = ET.parse(train_data_path + 'training_data.xml')\n",
    "train_root = training_xml.getroot()\n",
    "\n",
    "test_data_path = '../../data/svm_test_data/'\n",
    "test_data_src = '../../data/pan12-sexual-predator-identification-test-corpus-2012-05-21/'\n",
    "test_xml = ET.parse(test_data_src + 'pan12-sexual-predator-identification-test-corpus-2012-05-17.xml')\n",
    "test_root = test_xml.getroot()\n",
    "\n",
    "train_corpus, train_labels = get_features_labels(train_root, get_labels_dict(train_data_path))\n",
    "test_corpus, test_labels = get_features_labels(test_root, get_labels_dict(test_data_path))\n",
    "\n",
    "train_corpus_norm = []\n",
    "train_corpus_susp = []\n",
    "train_labels_norm = []\n",
    "train_labels_susp = []\n",
    "for index in range(len(train_corpus)):\n",
    "    if train_labels[index] == 1:\n",
    "        train_corpus_susp.append(train_corpus[index])\n",
    "        train_labels_susp.append(train_labels[index])\n",
    "    else:\n",
    "        train_corpus_norm.append(train_corpus[index])\n",
    "        train_labels_norm.append(train_labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.44709805\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import scipy\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "_ = vectorizer.fit_transform(train_corpus)\n",
    "X_train_norm = vectorizer.transform(train_corpus_norm)\n",
    "X_train_susp = vectorizer.transform(train_corpus_susp)\n",
    "X_test = vectorizer.transform(test_corpus)\n",
    "\n",
    "X_train_norm = scipy.sparse.csr_matrix(X_train_norm, dtype=np.float32)\n",
    "y_train_norm = np.array(train_labels_norm)\n",
    "X_train_susp = scipy.sparse.csr_matrix(X_train_susp, dtype=np.float32)\n",
    "y_train_susp = np.array(train_labels_susp)\n",
    "X_test = scipy.sparse.csr_matrix(X_test, dtype=np.float32)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "print(np.min(X_train_norm[:][0]))\n",
    "print(np.max(X_train_norm[:][0]))\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train_norm)\n",
    "X_train_susp = scaler.fit_transform(X_train_susp)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print(np.min(X_train_norm[:][0]))\n",
    "print(np.max(X_train_norm[:][0]))\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=87)\n",
    "# print(\"Train data shape:{}\\r\\nTest data shape:{}\".format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 119398)\t0.081819706\n",
      "  (0, 119332)\t0.14805734\n",
      "  (0, 119276)\t0.4289901\n",
      "  (0, 118973)\t0.041549932\n",
      "  (0, 118941)\t0.07015078\n",
      "  (0, 118779)\t0.4489788\n",
      "  (0, 118505)\t0.09728563\n",
      "  (0, 117862)\t0.047014344\n",
      "  (0, 116778)\t0.30102143\n",
      "  (0, 116483)\t0.08699928\n",
      "  (0, 115839)\t0.36217767\n",
      "  (0, 115808)\t0.094457544\n",
      "  (0, 115667)\t0.0963027\n",
      "  (0, 115510)\t0.047327537\n",
      "  (0, 115439)\t0.21800916\n",
      "  (0, 115238)\t0.22017342\n",
      "  (0, 115186)\t0.18904479\n",
      "  (0, 115042)\t0.18243727\n",
      "  (0, 114443)\t0.17148164\n",
      "  (0, 114358)\t0.15560317\n",
      "  (0, 113295)\t0.40773752\n",
      "  (0, 112762)\t0.077801585\n",
      "  (0, 112462)\t0.15320362\n",
      "  (0, 111681)\t0.10570731\n",
      "  (0, 111189)\t0.09465508\n",
      "  :\t:\n",
      "  (0, 24726)\t0.20149395\n",
      "  (0, 23331)\t0.26914763\n",
      "  (0, 22067)\t0.12671371\n",
      "  (0, 21787)\t0.070174195\n",
      "  (0, 21531)\t0.2797445\n",
      "  (0, 21493)\t0.12715293\n",
      "  (0, 21457)\t0.35625395\n",
      "  (0, 21160)\t0.17045741\n",
      "  (0, 20994)\t0.30733147\n",
      "  (0, 20764)\t0.13206111\n",
      "  (0, 19740)\t0.09832926\n",
      "  (0, 18206)\t0.28438988\n",
      "  (0, 17402)\t0.04014927\n",
      "  (0, 16970)\t0.5268748\n",
      "  (0, 16725)\t0.11428115\n",
      "  (0, 16117)\t0.04410973\n",
      "  (0, 15940)\t0.09106798\n",
      "  (0, 15693)\t0.16541399\n",
      "  (0, 14604)\t0.049082026\n",
      "  (0, 14564)\t0.12605098\n",
      "  (0, 14414)\t0.11577349\n",
      "  (0, 13130)\t0.37475684\n",
      "  (0, 6254)\t0.12291609\n",
      "  (0, 3249)\t0.050912797\n",
      "  (0, 1283)\t0.070635095\n"
     ]
    }
   ],
   "source": [
    "# print(X_train_norm[:][0])\n",
    "print(X_train_norm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121394\n",
      "WARNING:tensorflow:From c:\\ce 3a\\cs 480\\onlinegrooming\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Generative Adversarial Networks (GAN).\n",
    "Using generative adversarial networks (GAN) to generate digit images from a\n",
    "noise distribution.\n",
    "References:\n",
    "    - Generative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza,\n",
    "    B Xu, D Warde-Farley, S Ozair, Y. Bengio. Advances in neural information\n",
    "    processing systems, 2672-2680.\n",
    "    - Understanding the difficulty of training deep feedforward neural networks.\n",
    "    X Glorot, Y Bengio. Aistats 9, 249-256\n",
    "Links:\n",
    "    - [GAN Paper](https://arxiv.org/pdf/1406.2661.pdf).\n",
    "    - [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    - [Xavier Glorot Init](www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.../AISTATS2010_Glorot.pdf).\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Training Params\n",
    "num_steps = 10\n",
    "batch_size = 100 #128\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Network Params\n",
    "image_dim = X_train_norm.shape[1] #784 # 28*28 pixels\n",
    "gen_hidden_dim = 10 #256\n",
    "disc_hidden_dim = 10 #256\n",
    "noise_dim = X_train_norm.shape[1] # 100 # Noise data points\n",
    "print(X_train_norm.shape[1])\n",
    "\n",
    "# A custom initialization (see Xavier Glorot init)\n",
    "def glorot_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.))\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'gen_hidden1': tf.Variable(glorot_init([noise_dim, gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(glorot_init([gen_hidden_dim, image_dim])),\n",
    "    'disc1_hidden1': tf.Variable(glorot_init([image_dim, disc_hidden_dim])),\n",
    "    'disc1_out': tf.Variable(glorot_init([disc_hidden_dim, 1])),\n",
    "    'disc2_hidden1': tf.Variable(glorot_init([image_dim, disc_hidden_dim])),\n",
    "    'disc2_out': tf.Variable(glorot_init([disc_hidden_dim, 1])),\n",
    "}\n",
    "biases = {\n",
    "    'gen_hidden1': tf.Variable(tf.zeros([gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(tf.zeros([image_dim])),\n",
    "    'disc1_hidden1': tf.Variable(tf.zeros([disc_hidden_dim])),\n",
    "    'disc1_out': tf.Variable(tf.zeros([1])),\n",
    "    'disc2_hidden1': tf.Variable(tf.zeros([disc_hidden_dim])),\n",
    "    'disc2_out': tf.Variable(tf.zeros([1])),\n",
    "}\n",
    "\n",
    "\n",
    "# Generator\n",
    "def generator(x):\n",
    "    hidden_layer = tf.matmul(x, weights['gen_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['gen_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    out_layer = tf.matmul(hidden_layer, weights['gen_out'])\n",
    "    out_layer = tf.add(out_layer, biases['gen_out'])\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "def discriminator_SCI(x): # is D in paper\n",
    "    hidden_layer = tf.matmul(x, weights['disc1_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['disc1_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    out_layer = tf.matmul(hidden_layer, weights['disc1_out'])\n",
    "    out_layer = tf.add(out_layer, biases['disc1_out'])\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer\n",
    "\n",
    "def discriminator_gvr(x): # is D prime in paper, discriminator_generated_vs_real\n",
    "    hidden_layer = tf.matmul(x, weights['disc2_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['disc2_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    out_layer = tf.matmul(hidden_layer, weights['disc2_out'])\n",
    "    out_layer = tf.add(out_layer, biases['disc2_out'])\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , data.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i].toarray() for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "    data_shuffle = scipy.sparse.csr_matrix(data_shuffle)\n",
    "    print(data_shuffle.shape)\n",
    "    return data_shuffle, labels_shuffle\n",
    "#     print(len(data_shuffle[0]))\n",
    "#     return scipy.sparse.csr_matrix(data_shuffle), np.asarray(labels_shuffle)\n",
    "#     return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "# https://stackoverflow.com/questions/40896157/scipy-sparse-csr-matrix-to-tensorflow-sparsetensor-mini-batch-gradient-descent\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13802, 121394)\n",
      "(901, 121394)\n"
     ]
    }
   ],
   "source": [
    "# Build Networks\n",
    "# Network Inputs\n",
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')\n",
    "# disc_input_normal = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input_normal')\n",
    "# disc_input_real_susp = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input_real_susp')\n",
    "# # disc_input_fake_susp = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input_fake_susp')\n",
    "\n",
    "# tf.float32\n",
    "# https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428\n",
    "# x_norm = tf.sparse_placeholder(tf.float64)\n",
    "# x_susp = tf.sparse_placeholder(tf.float64)\n",
    "x_norm, x_susp = tf.placeholder(tf.float32, shape=[None, image_dim]), tf.placeholder(tf.float32, shape=[None, image_dim])\n",
    "dataset_norm = tf.data.Dataset.from_tensor_slices(x_norm).repeat().batch(batch_size)\n",
    "dataset_susp = tf.data.Dataset.from_tensor_slices(x_susp).repeat().batch(batch_size)\n",
    "# train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "# test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "iter_norm = dataset_norm.make_initializable_iterator()\n",
    "iter_susp = dataset_susp.make_initializable_iterator()\n",
    "features_norm = iter_norm.get_next()\n",
    "features_susp = iter_susp.get_next()\n",
    "\n",
    "# Build Generator Network\n",
    "gen_sample = generator(gen_input)\n",
    "\n",
    "# Build 2 Discriminator Networks (one from noise input, one from generated samples)\n",
    "# disc_SCI_normal = discriminator_SCI(disc_input_normal)\n",
    "# disc_SCI_susp_real = discriminator_SCI(disc_input_real_susp)\n",
    "# disc_SCI_susp_fake = discriminator_SCI(gen_sample)\n",
    "# disc_gvr_real = discriminator_gvr(disc_input_real_susp)\n",
    "# disc_gvr_fake = discriminator_gvr(gen_sample)\n",
    "\n",
    "disc_SCI_normal = discriminator_SCI(features_norm)\n",
    "disc_SCI_susp_real = discriminator_SCI(features_susp)\n",
    "disc_SCI_susp_fake = discriminator_SCI(gen_sample)\n",
    "disc_gvr_real = discriminator_gvr(features_susp)\n",
    "disc_gvr_fake = discriminator_gvr(gen_sample)\n",
    "\n",
    "\n",
    "# Build Loss\n",
    "gen_loss = -tf.reduce_mean(tf.log(disc_SCI_susp_fake) + tf.log(disc_gvr_fake))\n",
    "disc_SCI_loss = -tf.reduce_mean(tf.log(disc_SCI_normal) + tf.log(1. - disc_SCI_susp_real) + tf.log(1. - disc_SCI_susp_fake))\n",
    "disc_gvr_loss = -tf.reduce_mean(tf.log(disc_gvr_real) + tf.log(1. - disc_gvr_fake))\n",
    "\n",
    "# Build Optimizers\n",
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Training Variables for each optimizer\n",
    "# By default in TensorFlow, all variables are updated by each optimizer, so we\n",
    "# need to precise for each one of them the specific variables to update.\n",
    "# Generator Network Variables\n",
    "gen_vars = [weights['gen_hidden1'], weights['gen_out'],\n",
    "            biases['gen_hidden1'], biases['gen_out']]\n",
    "# Discriminator Network Variables\n",
    "disc_SCI_vars = [weights['disc1_hidden1'], weights['disc1_out'],\n",
    "            biases['disc1_hidden1'], biases['disc1_out']]\n",
    "disc_gvr_vars = [weights['disc2_hidden1'], weights['disc2_out'],\n",
    "            biases['disc2_hidden1'], biases['disc2_out']]\n",
    "\n",
    "# Create training operations\n",
    "train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "train_disc_SCI = optimizer_disc.minimize(disc_SCI_loss, var_list=disc_SCI_vars)\n",
    "train_disc_gvr = optimizer_disc.minimize(disc_gvr_loss, var_list=disc_gvr_vars)\n",
    "\n",
    "print(X_train_norm.shape)\n",
    "print(X_train_susp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Generator Loss: 1.490234, Disc SCI Loss: 2.102442, Disc gvr Loss: 1.277664\n",
      "\n",
      "Step 2: Generator Loss: inf, Disc SCI Loss: 1.383820, Disc gvr Loss: 0.695601\n",
      "\n",
      "Step 3: Generator Loss: nan, Disc SCI Loss: nan, Disc gvr Loss: nan\n",
      "\n",
      "Step 4: Generator Loss: nan, Disc SCI Loss: nan, Disc gvr Loss: nan\n",
      "\n",
      "Step 5: Generator Loss: nan, Disc SCI Loss: nan, Disc gvr Loss: nan\n",
      "\n",
      "Step 6: Generator Loss: nan, Disc SCI Loss: nan, Disc gvr Loss: nan\n",
      "\n",
      "Step 7: Generator Loss: nan, Disc SCI Loss: nan, Disc gvr Loss: nan\n",
      "\n",
      "Step 8: Generator Loss: nan, Disc SCI Loss: nan, Disc gvr Loss: nan\n",
      "\n",
      "Step 9: Generator Loss: nan, Disc SCI Loss: nan, Disc gvr Loss: nan\n",
      "\n",
      "Step 10: Generator Loss: nan, Disc SCI Loss: nan, Disc gvr Loss: nan\n",
      "Finished Training\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4db2dffec9d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished Training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# test SCI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisc_SCI_normal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ce 3a\\cs 480\\onlinegrooming\\venv\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mtodense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    847\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \"\"\"\n\u001b[1;32m--> 849\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ce 3a\\cs 480\\onlinegrooming\\venv\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ce 3a\\cs 480\\onlinegrooming\\venv\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1185\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    sess.run([iter_norm.initializer, iter_susp.initializer], feed_dict={ x_norm: X_train_norm.todense(), x_susp: X_train_susp.todense()})\n",
    "    \n",
    "    for i in range(1, num_steps+1):\n",
    "        # Prepare Data\n",
    "        # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "#         batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "#         batch_x_norm, _ = next_batch(batch_size, X_train_norm, y_train_norm)\n",
    "#         batch_x_susp, _ = next_batch(batch_size, X_train_susp, y_train_susp)\n",
    "#         print(batch_x_norm.shape)\n",
    "#         print(batch_x_susp.shape)\n",
    "        # Generate noise to feed to the generator\n",
    "        z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n",
    "        print()\n",
    "        # Train\n",
    "#         feed_dict = {disc_input: batch_x, gen_input: z}\n",
    "        feed_dict = {gen_input: z}\n",
    "        _, _, _, gl, dSCIl, dgvrl = sess.run([train_gen, train_disc_SCI, train_disc_gvr, gen_loss, disc_SCI_loss, disc_gvr_loss],\n",
    "                                feed_dict=feed_dict)\n",
    "#         if i % 1000 == 0 or i == 1:\n",
    "        print('Step %i: Generator Loss: %f, Disc SCI Loss: %f, Disc gvr Loss: %f' % (i, gl, dSCIl, dgvrl))\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    # test SCI\n",
    "    sess.run(iter_norm.initializer, feed_dict={ x_norm: X_test.todense()})\n",
    "    y_pred = sess.run(disc_SCI_normal)\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "#     # Generate images from noise, using the generator network.\n",
    "#     f, a = plt.subplots(4, 10, figsize=(10, 4))\n",
    "#     for i in range(10):\n",
    "#         # Noise input.\n",
    "#         z = np.random.uniform(-1., 1., size=[4, noise_dim])\n",
    "#         g = sess.run([gen_sample], feed_dict={gen_input: z})\n",
    "#         g = np.reshape(g, newshape=(4, 28, 28, 1))\n",
    "#         # Reverse colours for better display\n",
    "#         g = -1 * (g - 1)\n",
    "#         for j in range(4):\n",
    "#             # Generate image from noise. Extend to 3 channels for matplot figure.\n",
    "#             img = np.reshape(np.repeat(g[j][:, :, np.newaxis], 3, axis=2),\n",
    "#                              newshape=(28, 28, 3))\n",
    "#             a[j][i].imshow(img)\n",
    "\n",
    "#     f.show()\n",
    "#     plt.draw()\n",
    "#     plt.waitforbuttonpress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
